{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selector Group Chat\n",
    "SelectorGroupChat implements a team where participants take turns broadcasting messages to all other members. A generative model (e.g., an LLM) selects the next speaker based on the shared context, enabling dynamic, context-aware collaboration.\n",
    "\n",
    "Key features include:\n",
    "- Model-based speaker selection\n",
    "- Configurable participant roles and descriptions\n",
    "- Prevention of consecutive turns by the same speaker (optional)\n",
    "- Customizable selection prompting\n",
    "- Customizable selection function to override the default model-based selection\n",
    "\n",
    "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:\n",
    "1. The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. You can override the model by providing a custom selection function.\n",
    "2. The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.\n",
    "3. The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.\n",
    "4. When the conversation ends, the team returns the TaskResult containing the conversation history from this task.\n",
    "\n",
    "Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling reset()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Resources Needed\n",
    "1. Azure OpenAI\n",
    "    - Deploy GPT-4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create the Dalle client\n",
    "dalle_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "dalle_deployment_name = \"dall-e-3\"\n",
    "\n",
    "# Create the Vision client\n",
    "vision_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key, \n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")\n",
    "vision_deployment_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import requests\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define a tool\n",
    "# This function calls the Dalle-3 image generator given the prompt and displays the generated image.\n",
    "def generate_image(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI Dall-e 3 model to generate an image from a text prompt.\n",
    "    Executes the call to the Azure OpenAI Dall-e 3 image creator, saves the file into the local directory, and displays the image.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Dalle Assistant Message: Creating the image ...\")\n",
    "\n",
    "    response = dalle_client.images.generate(\n",
    "        model=dalle_deployment_name, prompt=prompt, size=\"1024x1024\", quality=\"standard\", n=1\n",
    "    )\n",
    "\n",
    "    # Retrieve the image URL from the response (assuming response structure)\n",
    "    image_url = response.data[0].url\n",
    "\n",
    "    # Open the image from the URL and save it to a temporary file.\n",
    "    im = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "    # Define the filename and path where the image should be saved.\n",
    "    filename = \"temp.jpg\"\n",
    "    local_path = Path(filename)\n",
    "\n",
    "    # Save the image.\n",
    "    im.save(local_path)\n",
    "\n",
    "    # Get the absolute path of the saved image.\n",
    "    full_path = str(local_path.absolute())\n",
    "\n",
    "    img = cv2.imread(\"temp.jpg\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    # Convert the image from BGR to RGB for displaying with matplotlib,\n",
    "    # because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image with matplotlib.\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis(\"off\")  # Turn off axis labels.\n",
    "    plt.show()\n",
    "\n",
    "    # Return the full path of the saved image.\n",
    "    print(\"Dalle Assistant Message: \" + full_path)\n",
    "    return \"Image generated successfully and store in the local file system. You can now use this image to analyze it with the vision_assistant\"\n",
    "\n",
    "# Define an agent\n",
    "dalle_assistant = AssistantAgent(\n",
    "    name=\"dalle_assistant\",\n",
    "    description=\"This agent calls the Azure OpenAI Dall-e 3 model to generate an image from a text prompt coming from the user or a vision assistant.\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[generate_image],\n",
    "    system_message=\"\"\"\n",
    "    As a premier AI specializing in image generation, you possess the expertise to craft precise visuals based on given prompts. \n",
    "    It is essential that you diligently generate the requested image, ensuring its accuracy and alignment with the user's specifications, \n",
    "    prior to delivering a response.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the image\n",
    "def analyze_image() -> str:\n",
    "    \"\"\"\n",
    "    Call the Azure OpenAI Vision model to analyze and critic an image and return the result.The resulting output should be a new prompt for dall-e that enhances the image based on the criticism and analysis\n",
    "    \"\"\"\n",
    "    print(\"Vision Assistant Message: \" + \"Analyzing the image...\")\n",
    "\n",
    "    import base64\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create a Path object for the image file\n",
    "    image_path = Path(\"temp.jpg\")\n",
    "\n",
    "    # Using a context manager to open the file with Path.open()\n",
    "    with image_path.open(\"rb\") as image_file:\n",
    "        base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    content_images = [\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "        for base64_image in [base64_image]\n",
    "    ]\n",
    "    response = vision_client.chat.completions.create(\n",
    "        model=vision_deployment_name,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze and critic this image and generate a new enhanced prompt for Dall-e with the criticism and analysis.\",\n",
    "                    },\n",
    "                    *content_images,\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "    print(\"Vision Assistant Message: \" + response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Creating the vision assistant agent\n",
    "vision_assistant = AssistantAgent(\n",
    "    name=\"vision_assistant\",\n",
    "    description=\"This agent analyzes and critic an image and return the result. The resulting output should be a new prompt for dalle assistant that enhances the image based on the criticism and analysis\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[analyze_image],\n",
    "    system_message=\"\"\"\n",
    "    As a leading AI expert in image analysis, you excel at scrutinizing and offering critiques to refine and improve images. \n",
    "    Your task is to thoroughly analyze an image, ensuring that all essential assessments are completed with precision \n",
    "    before you provide feedback to the user. You have access to the local file system where the image is stored.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user proxy agent.\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the planning agent\n",
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=az_model_client,\n",
    "    system_message=f\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        Dalle Assistant: An AI specializing in image generation.\n",
    "        Vision Assistant: An AI expert in image analysis.\n",
    "        User Proxy: The user\n",
    "\n",
    "    You will follow this sequence:\n",
    "        Dalle Assistant will generate an image based on the initial user prompt and display it for review.\n",
    "        Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        Dalle Assistant will generate an image based on the Vision Assistant prompt and display it for review.\n",
    "        Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        Dalle Assistant will generate an image based on the Vision Assistant prompt and display it for review.\n",
    "        Vision Assistant will analyze the image and provide a new prompt for Dalle Assistant to generate a new image based on the new prompt.\n",
    "        User Proxy will provide feedback on process.\n",
    "        \n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the team\n",
    "Let’s create the team with two termination conditions: \n",
    "- TextMentionTermination to end the conversation when the User Proxy sends “APPROVE”\n",
    "- MaxMessageTermination to limit the conversation to avoid infinite loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"APPROVE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=50)\n",
    "termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, dalle_assistant, vision_assistant, user_proxy],\n",
    "    model_client=az_model_client,\n",
    "    termination_condition=termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Task and Run the Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Generate an image of a boat drifting in the water and analyze it and enhance the image\"\n",
    "# Use asyncio.run(...) if you are running this in a script.\n",
    "response = await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing whole response per agent source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape code for bold text\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# ANSI escape code for red text\n",
    "red_start = \"\\033[31m\"\n",
    "red_end = \"\\033[0m\"\n",
    "\n",
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Selector Function\n",
    "Often times we want better control over the selection process. To this end, we can set the selector_func argument with a custom selector function to override the default model-based selection. For instance, we want the Planning Agent to speak immediately after any specialized agent to check the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function determines whether the last message in the sequence was sent by the planning_agent. \n",
    "# If it wasn't, the function returns the name of the planning_agent; otherwise, it returns None.\n",
    "# Returning None from the custom selector function will use the default model-based selection.\n",
    "def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:\n",
    "    if messages[-1].source != planning_agent.name:\n",
    "        return planning_agent.name\n",
    "    return None\n",
    "\n",
    "\n",
    "# Reset the previous team and run the chat again with the selector function.\n",
    "await team.reset()\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, dalle_assistant, vision_assistant, user_proxy],\n",
    "    model_client=az_model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_func=selector_func,\n",
    ")\n",
    "\n",
    "response = await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for messages in response.messages:\n",
    "    source = messages.source\n",
    "    print(f\"{bold_start}{red_start}{source}{bold_end}{red_end}\")\n",
    "    print(messages.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
