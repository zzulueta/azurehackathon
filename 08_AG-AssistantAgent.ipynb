{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGen Assistant Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Resources Needed\n",
    "1. Azure OpenAI\n",
    "    - Deploy GPT-4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"autogen-agentchat==0.4.0.dev13\" \"autogen-ext[openai,azure,docker]==0.4.0.dev13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Azure Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure OpenAI Client\n",
    "Using the model client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Create the token provider\n",
    "#token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "az_model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_deployment=azure_openai_deployment,\n",
    "    model=azure_openai_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    # azure_ad_token_provider=token_provider,  # Optional if you choose key-based authentication.\n",
    "    api_key=azure_openai_key, # For key-based authentication.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an Agent\n",
    "AssistantAgent is a built-in agent that uses a language model and has the ability to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "# Define a tool\n",
    "async def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny. You are supposed to call the weather API here.\"\n",
    "\n",
    "async def get_news(city: str) -> str:\n",
    "    return f\"The news in {city} is this. You are supposed to call the news API here.\"\n",
    "\n",
    "# Define an agent\n",
    "weather_agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    description=\"This agent provides weather and news information.\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[get_weather, get_news],\n",
    "    system_message=\"\"\"\n",
    "    You are agent that assists with weather and news information.\n",
    "    If you are given an image, you will identify the city in the image and provide the information. \n",
    "    If you are asked about other topics, tell the user you can only provide weather and news information.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Message\n",
    "Create a text message, which accepts a string content and a string source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "text_message = TextMessage(content=\"What's the weather in Chicago\", source=\"User\")\n",
    "\n",
    "# Run the team and stream messages to the console.\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "text_message = TextMessage(content=\"What's the latest news in San Francisco\", source=\"User\")\n",
    "\n",
    "# Run the team and stream messages to the console.\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiModal Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.messages import MultiModalMessage\n",
    "from autogen_core import Image as AGImage\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "img_path = \"Data/images/empirestate.jpg\"\n",
    "\n",
    "# Load the image using OpenCV.\n",
    "img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "# Convert the image from BGR to RGB for displaying with matplotlib, because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with matplotlib.\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")  # Turn off axis labels.\n",
    "plt.show()\n",
    "\n",
    "raw_image = Image.open(img_path)\n",
    "# Create an AGImage object from the raw image.\n",
    "img = AGImage(raw_image)\n",
    "multi_modal_message = MultiModalMessage(content=[\"What's the weather in this image?\", img], source=\"User\")\n",
    "\n",
    "# Run the team and stream messages to the console.\n",
    "stream = weather_agent.run_stream(task=multi_modal_message)\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents have State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_message = TextMessage(content=\"What were the cities again that I asked about the weather?\", source=\"User\")\n",
    "\n",
    "# Run the team and stream messages to the console.\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. on_messages() method returns a Response that contains the following attributes:\n",
    "- chat_message: contains agent’s final response \n",
    "- inner_messages: stores the agent’s “thought process” that led to the final response.\n",
    "2. on_messages() will update the internal state of the agent – it will add the messages to the agent’s history. \n",
    "- You should call this method with new messages.\n",
    "- You should NOT repeatedly call this method with the same messages or the complete history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"Data/images/mtfuji.jpg\"\n",
    "\n",
    "img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "# Convert the image from BGR to RGB for displaying with matplotlib, because OpenCV uses BGR by default and matplotlib expects RGB.\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with matplotlib.\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis(\"off\")  # Turn off axis labels.\n",
    "plt.show()\n",
    "\n",
    "raw_image = Image.open(img_path )\n",
    "# Create an AGImage object from the raw image.\n",
    "img = AGImage(raw_image)\n",
    "multi_modal_message = MultiModalMessage(content=[\"What's the city where this image is located? Provide the weather there.\", img], source=\"User\")\n",
    "\n",
    "cancellation_token=CancellationToken()\n",
    "response = await weather_agent.on_messages(\n",
    "    [multi_modal_message],\n",
    "    cancellation_token=cancellation_token,\n",
    ")\n",
    "# contains the agent's \"thought process\"\n",
    "print(\"Thought process: \")\n",
    "print(response.inner_messages)\n",
    "print(\"\\n\")\n",
    "# contains the agent's final response\n",
    "print(\"Final Response: \")\n",
    "print(response.chat_message)\n",
    "print(\"\\n\")\n",
    "print(\"Final Response Content: \")\n",
    "print(response.chat_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Messages\n",
    "We can also stream each message as it is generated by the agent by using the on_messages_stream() method, and use Console to print the messages as they appear to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_message = TextMessage(content=\"What were the cities again that I asked about the weather?\", source=\"User\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "cancellation_token=CancellationToken()\n",
    "await Console(weather_agent.on_messages_stream(\n",
    "        [text_message],\n",
    "        cancellation_token=cancellation_token,)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Agent to Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancellation_token=CancellationToken()\n",
    "# Reset the agent\n",
    "await weather_agent.on_reset(cancellation_token=cancellation_token)\n",
    "\n",
    "text_message = TextMessage(content=\"What were the cities again that I asked about the weather?\", source=\"User\")\n",
    "response = await weather_agent.on_messages(\n",
    "        [text_message],\n",
    "        cancellation_token=cancellation_token,\n",
    "    )\n",
    "print(response.chat_message.content)\n",
    "\n",
    "# Reset the agent again\n",
    "await weather_agent.on_reset(cancellation_token=cancellation_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model Context\n",
    "AssistantAgent has a model_context parameter that can be used to pass in a ChatCompletionContext object. This allows the agent to use different model contexts, such as BufferedChatCompletionContext to limit the context sent to the model.\n",
    "\n",
    "By default, AssistantAgent uses the UnboundedChatCompletionContext which sends the full conversation history to the model. To limit the context to the last n messages, you can use the BufferedChatCompletionContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "# Define a tool\n",
    "async def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is 73 degrees and Sunny.\"\n",
    "\n",
    "# Define an agent\n",
    "weather_agent = AssistantAgent(\n",
    "    name=\"weather_agent\",\n",
    "    description=\"This agent provides weather information.\",\n",
    "    model_client=az_model_client,\n",
    "    tools=[get_weather],\n",
    "    system_message=\"\"\"\n",
    "    You are agent that assists with weather information.\n",
    "    \"\"\",\n",
    "    model_context=BufferedChatCompletionContext(buffer_size=8),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_message = TextMessage(content=\"What is the weather in Chicago?\", source=\"User\")\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)\n",
    "print(\"\\n\")\n",
    "\n",
    "text_message = TextMessage(content=\"How about in New York?\", source=\"User\")\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)\n",
    "print(\"\\n\")\n",
    "\n",
    "text_message = TextMessage(content=\"What's it like in Tokyo?\", source=\"User\")\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_message = TextMessage(content=\"What was the weather in the first city again?\", source=\"User\")\n",
    "stream = weather_agent.run_stream(task=text_message)\n",
    "await Console(stream)\n",
    "print(\"\\n\")\n",
    "\n",
    "## New York is the first city in the context given that each interaction was 4 messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "state = await weather_agent.save_state()\n",
    "\n",
    "# (Optional) Write state to disk.\n",
    "with open(\"assistant_state.json\", \"w\") as f:\n",
    "    json.dump(state, f)\n",
    "\n",
    "# (Optional) Load it back from disk.\n",
    "with open(\"assistant_state.json\", \"r\") as f:\n",
    "    state = json.load(f)\n",
    "    print(state) # Inspect the state, which contains the chat history.\n",
    "\n",
    "# Carry on the chat.\n",
    "response = await weather_agent.on_messages([TextMessage(content=\"What is the weather in Rome?\", source=\"user\")], cancellation_token=CancellationToken())\n",
    "print(response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry on the same chat again.\n",
    "response = await weather_agent.on_messages([TextMessage(content=\"What was the weather in the first city again?\", source=\"user\")], cancellation_token=CancellationToken())\n",
    "print(response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the state, resulting the agent to revert to the previous state before the last message.\n",
    "await weather_agent.load_state(state)\n",
    "\n",
    "# Carry on the same chat again.\n",
    "response = await weather_agent.on_messages([TextMessage(content=\"What was the weather in the first city again?\", source=\"user\")], cancellation_token=CancellationToken())\n",
    "print(response.chat_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
